{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41761ae0-99a5-4bac-bba9-74c7a8aa0d85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import mlx_whisper\n",
    "#from transformers import pipeline\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "from tqdm.notebook import tqdm\n",
    "import time  # ãƒ‡ãƒ¢ç”¨ï¼ˆå®Ÿéš›ã®å‡¦ç†ã®é€²è¡ŒãŒåˆ†ã‹ã‚‹ã‚ˆã†ã«ã™ã‚‹ãŸã‚ï¼‰\n",
    "\n",
    "# -------------------------\n",
    "# 1. éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ–‡å­—èµ·ã“ã—\n",
    "# -------------------------\n",
    "print(\"ğŸ“¢ æ–‡å­—èµ·ã“ã—ã‚’é–‹å§‹ã—ã¾ã™...\")\n",
    "\n",
    "# tqdm ã‚’ä½¿ç”¨ã—ã¦é€²æ—è¡¨ç¤ºï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰\n",
    "for _ in tqdm(range(3), desc=\"éŸ³å£°å‡¦ç†ä¸­...\"):\n",
    "    time.sleep(1)  # å®Ÿéš›ã®å‡¦ç†æ™‚é–“ã®ä»£ã‚ã‚Šã«ã‚¹ãƒªãƒ¼ãƒ—ã‚’æŒ¿å…¥\n",
    "\n",
    "# éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹\n",
    "audio_data = 'test_recording.m4a'\n",
    "\n",
    "# å°‚é–€ç”¨èªãƒ»å›ºæœ‰åè©ã®ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\n",
    "custom_prompt = (\n",
    "    \"ä»¥ä¸‹ã®å°‚é–€ç”¨èªãƒ»å›ºæœ‰åè©ã¯å¿…ãšä¸‹è¨˜ã®è¡¨è¨˜ã§æ–‡å­—èµ·ã“ã—ã—ã¦ãã ã•ã„ã€‚\\n\"\n",
    "    \"ãƒ»ã€AIã€ã¯ã€ã‚¨ãƒ¼ã‚¢ã‚¤ã€\\n\"\n",
    "    \"ãƒ»ã€Deep Learningã€ã¯ã€ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã€\\n\"\n",
    "    \"ãƒ»ã€GPTã€ã¯ã€ã‚¸ãƒ¼ãƒ”ãƒ¼ãƒ†ã‚£ãƒ¼ã€\\n\"\n",
    ")\n",
    "\n",
    "# æ–‡å­—èµ·ã“ã—å®Ÿè¡Œ\n",
    "result = mlx_whisper.transcribe(\n",
    "    audio_data,\n",
    "    path_or_hf_repo=\"mlx-community/whisper-large-v3-mlx\",\n",
    "    prompt=custom_prompt\n",
    ")\n",
    "\n",
    "# **æ–‡ã®æ•´å½¢å‡¦ç†**\n",
    "def format_text(text):\n",
    "    import re\n",
    "    text = re.sub(r\"([ã€‚ï¼ï¼Ÿ])\", r\"\\1\\n\", text)  # å¥èª­ç‚¹ã§æ”¹è¡Œ\n",
    "    text = re.sub(r\"ã€\", \"ã€ \", text)  # èª­ç‚¹ã®å¾Œã«ã‚¹ãƒšãƒ¼ã‚¹ã‚’å…¥ã‚Œã‚‹\n",
    "    return text.strip()\n",
    "transcription_text = result['text']\n",
    "# æ–‡å­—èµ·ã“ã—çµæœã‚’ä¿å­˜\n",
    "with open(\"transcription.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(transcription_text)\n",
    "\n",
    "print(\"âœ… æ–‡å­—èµ·ã“ã—ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")\n",
    "\n",
    "# -------------------------\n",
    "# 2. æ–‡å­—èµ·ã“ã—çµæœã®è¦ç´„\n",
    "# -------------------------\n",
    "print(\"ğŸ“„ è¦ç´„ã‚’ä½œæˆä¸­...\")\n",
    "\n",
    "# tqdm ã‚’ä½¿ç”¨ã—ã¦é€²æ—è¡¨ç¤º\n",
    "for _ in tqdm(range(3), desc=\"è¦ç´„å‡¦ç†ä¸­...\"):\n",
    "    time.sleep(1)\n",
    "\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ã‚’æŒ‡å®šï¼ˆäº‹å‰ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ï¼‰\n",
    "model_path = \"./models/bart-large-cnn\"\n",
    "\n",
    "# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "\n",
    "# è¦ç´„ç”¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆï¼ˆlocal_files_only ã¯å‰Šé™¤ï¼‰\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# è¦ç´„å®Ÿè¡Œ\n",
    "summary_result = summarizer(transcription_text, max_length=150, min_length=40, do_sample=False)\n",
    "summary_text = summary_result[0]['summary_text']\n",
    "\n",
    "# è¦ç´„çµæœã‚’ä¿å­˜\n",
    "with open(\"summary.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(summary_text)\n",
    "\n",
    "print(\"âœ… è¦ç´„ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")\n",
    "\n",
    "# -------------------------\n",
    "# 3. è‹±èªç¿»è¨³ï¼ˆæ–‡å­—èµ·ã“ã—çµæœãƒ»è¦ç´„ï¼‰\n",
    "# -------------------------\n",
    "print(\"ğŸŒ è‹±èªç¿»è¨³ã‚’é–‹å§‹ã—ã¾ã™...\")\n",
    "\n",
    "# tqdm ã‚’ä½¿ç”¨ã—ã¦é€²æ—è¡¨ç¤º\n",
    "for _ in tqdm(range(3), desc=\"ç¿»è¨³å‡¦ç†ä¸­...\"):\n",
    "    time.sleep(1)\n",
    "\n",
    "# ç¿»è¨³ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³\n",
    "translator = pipeline(\n",
    "    \"translation\",\n",
    "    model=\"Helsinki-NLP/opus-mt-ja-en\"\n",
    ")\n",
    "\n",
    "# æ–‡å­—èµ·ã“ã—çµæœã®è‹±èªç¿»è¨³\n",
    "english_transcription = translator(transcription_text, max_length=512)[0]['translation_text']\n",
    "with open(\"transcription_en.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(english_transcription)\n",
    "\n",
    "# è¦ç´„çµæœã®è‹±èªç¿»è¨³\n",
    "english_summary = translator(summary_text, max_length=512)[0]['translation_text']\n",
    "with open(\"summary_en.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(english_summary)\n",
    "\n",
    "print(\"âœ… è‹±èªç¿»è¨³ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")\n",
    "\n",
    "# -------------------------\n",
    "# 4. o3ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä»˜ãæ–‡æ›¸ã®ä½œæˆ\n",
    "# -------------------------\n",
    "print(\"ğŸ“‘ o3ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä»˜ãæ–‡æ›¸ã‚’ä½œæˆä¸­...\")\n",
    "\n",
    "o3_prompt = (\n",
    "    \"ä»¥ä¸‹ã¯è«–æ–‡ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ–‡å­—èµ·ã“ã—ã—ãŸå†…å®¹ã§ã™ã€‚\\n\"\n",
    "    \"å†…å®¹ã«ã¤ã„ã¦ã€o3ã®è¦–ç‚¹ã‹ã‚‰æ„è¦‹ã‚„æ”¹å–„ç‚¹ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚\\n\\n\"\n",
    ")\n",
    "o3_document = o3_prompt + transcription_text\n",
    "\n",
    "# o3ç”¨æ–‡æ›¸ã‚’ä¿å­˜\n",
    "with open(\"o3_prompt_document.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(o3_document)\n",
    "\n",
    "print(\"âœ… o3ç”¨ã®æ–‡æ›¸ä½œæˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")\n",
    "\n",
    "# -------------------------\n",
    "# å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸\n",
    "# -------------------------\n",
    "print(\"ğŸ‰ ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda469d3-c27c-47c9-9681-e10f0fe8b0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18eead2e-0481-42be-93c1-8acc6a0a834a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!poetry add sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e748d9a9-7750-49c1-8436-d06656932ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¢ æ–‡å­—èµ·ã“ã—ã‚’é–‹å§‹ã—ã¾ã™...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d521a5262d94766bf25a4bf32f09010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "éŸ³å£°å‡¦ç†ä¸­...:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ–‡å­—èµ·ã“ã—ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\n",
      "ğŸ“„ è¦ç´„ã‚’ä½œæˆä¸­...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37cc76f24dc341839f2b8addb9ce33ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "è¦ç´„å‡¦ç†ä¸­...:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… è¦ç´„ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\n",
      "ğŸŒ è‹±èªç¿»è¨³ã‚’é–‹å§‹ã—ã¾ã™...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "281e71be6f4f4c8ca5e038a181041625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ç¿»è¨³å‡¦ç†ä¸­...:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hajime/Library/Caches/pypoetry/virtualenvs/whisper-meeting-TBf8Z80e-py3.12/lib/python3.12/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Device set to use mps:0\n",
      "Your input_length: 510 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… è‹±èªç¿»è¨³ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\n",
      "ğŸ“‘ o3ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä»˜ãæ–‡æ›¸ã‚’ä½œæˆä¸­...\n",
      "âœ… o3ç”¨ã®æ–‡æ›¸ä½œæˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚\n",
      "ğŸ‰ ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import mlx_whisper\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "\n",
    "# å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€ã®ä½œæˆ\n",
    "output_dir = \"outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "# -------------------------\n",
    "# 1. éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ–‡å­—èµ·ã“ã—\n",
    "# -------------------------\n",
    "print(\"ğŸ“¢ æ–‡å­—èµ·ã“ã—ã‚’é–‹å§‹ã—ã¾ã™...\")\n",
    "\n",
    "for _ in tqdm(range(3), desc=\"éŸ³å£°å‡¦ç†ä¸­...\"):\n",
    "    time.sleep(1)\n",
    "\n",
    "# éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹\n",
    "audio_data = \"test_recording.m4a\"\n",
    "\n",
    "\n",
    "# å°‚é–€ç”¨èªãƒ»å›ºæœ‰åè©ãƒªã‚¹ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€\n",
    "def load_custom_terms(file_path):\n",
    "    terms = []\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if \",\" in line:\n",
    "                    original, replacement = line.strip().split(\",\")\n",
    "                    terms.append(f\"ãƒ»ã€{original.strip()}ã€ã¯ã€{replacement.strip()}ã€\")\n",
    "    return \"\\n\".join(terms)\n",
    "\n",
    "\n",
    "# ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ\n",
    "terms_file = \"custom_terms.txt\"\n",
    "custom_terms = load_custom_terms(terms_file)\n",
    "\n",
    "custom_prompt = (\n",
    "    \"ä»¥ä¸‹ã®å°‚é–€ç”¨èªãƒ»å›ºæœ‰åè©ã¯å¿…ãšä¸‹è¨˜ã®è¡¨è¨˜ã§æ–‡å­—èµ·ã“ã—ã—ã¦ãã ã•ã„ã€‚\\n\"\n",
    "    + custom_terms\n",
    ")\n",
    "\n",
    "# æ–‡å­—èµ·ã“ã—å®Ÿè¡Œ\n",
    "result = mlx_whisper.transcribe(\n",
    "    audio_data,\n",
    "    path_or_hf_repo=\"mlx-community/whisper-large-v3-mlx\",\n",
    "    prompt=custom_prompt,\n",
    ")\n",
    "\n",
    "\n",
    "# **æ–‡ã®æ•´å½¢å‡¦ç†**\n",
    "def format_text(text):\n",
    "    import re\n",
    "\n",
    "    text = re.sub(r\"([ã€‚ï¼ï¼Ÿ])\", r\"\\1\\n\", text)  # å¥èª­ç‚¹ã§æ”¹è¡Œ\n",
    "    text = re.sub(r\"ã€\", \"ã€ \", text)  # èª­ç‚¹ã®å¾Œã«ã‚¹ãƒšãƒ¼ã‚¹ã‚’å…¥ã‚Œã‚‹\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "transcription_text = format_text(result[\"text\"])\n",
    "\n",
    "# æ–‡å­—èµ·ã“ã—çµæœã‚’ä¿å­˜\n",
    "\n",
    "# æ–‡å­—èµ·ã“ã—çµæœã‚’ä¿å­˜\n",
    "transcription_path = os.path.join(output_dir, \"transcription.txt\")\n",
    "with open(transcription_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(transcription_text)\n",
    "\n",
    "print(\"âœ… æ–‡å­—èµ·ã“ã—ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")\n",
    "\n",
    "# -------------------------\n",
    "# 2. æ–‡å­—èµ·ã“ã—çµæœã®è¦ç´„\n",
    "# -------------------------\n",
    "print(\"ğŸ“„ è¦ç´„ã‚’ä½œæˆä¸­...\")\n",
    "\n",
    "for _ in tqdm(range(3), desc=\"è¦ç´„å‡¦ç†ä¸­...\"):\n",
    "    time.sleep(1)\n",
    "\n",
    "summarizer_path = \"./models/bart-large-cnn\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(summarizer_path, local_files_only=True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(summarizer_path, local_files_only=True)\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "summary_result = summarizer(\n",
    "    transcription_text, max_length=150, min_length=40, do_sample=False\n",
    ")\n",
    "summary_text = format_text(summary_result[0][\"summary_text\"])\n",
    "\n",
    "summary_path = os.path.join(output_dir, \"summary.txt\")\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(summary_text)\n",
    "\n",
    "print(\"âœ… è¦ç´„ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")\n",
    "\n",
    "# -------------------------\n",
    "# 3. è‹±èªç¿»è¨³ï¼ˆæ–‡å­—èµ·ã“ã—çµæœãƒ»è¦ç´„ï¼‰\n",
    "# -------------------------\n",
    "# -------------------------\n",
    "# 3. è‹±èªç¿»è¨³ï¼ˆæ–‡å­—èµ·ã“ã—çµæœãƒ»è¦ç´„ï¼‰\n",
    "# -------------------------\n",
    "print(\"ğŸŒ è‹±èªç¿»è¨³ã‚’é–‹å§‹ã—ã¾ã™...\")\n",
    "\n",
    "# tqdm ã‚’ä½¿ç”¨ã—ã¦é€²æ—è¡¨ç¤º\n",
    "for _ in tqdm(range(3), desc=\"ç¿»è¨³å‡¦ç†ä¸­...\"):\n",
    "    time.sleep(1)\n",
    "\n",
    "# ç¿»è¨³ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-ja-en\")\n",
    "\n",
    "# æ–‡å­—èµ·ã“ã—çµæœã®è‹±èªç¿»è¨³\n",
    "english_transcription = translator(transcription_text, max_length=512)[0][\n",
    "    \"translation_text\"\n",
    "]\n",
    "# with open(\"transcription_en.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#    f.write(english_transcription)\n",
    "\n",
    "translation_en_path = os.path.join(output_dir, \"translation_en.txt\")\n",
    "with open(translation_en_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(english_transcription)\n",
    "\n",
    "# è¦ç´„çµæœã®è‹±èªç¿»è¨³\n",
    "english_summary = translator(summary_text, max_length=512)[0][\"translation_text\"]\n",
    "# with open(\"summary_en.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#    f.write(english_summary)\n",
    "\n",
    "summary_en_path = os.path.join(output_dir, \"summary_en.txt\")\n",
    "with open(summary_en_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(english_summary)\n",
    "\n",
    "print(\"âœ… è‹±èªç¿»è¨³ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")\n",
    "\n",
    "# -------------------------\n",
    "# 4. o3ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä»˜ãæ–‡æ›¸ã®ä½œæˆ\n",
    "# -------------------------\n",
    "print(\"ğŸ“‘ o3ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä»˜ãæ–‡æ›¸ã‚’ä½œæˆä¸­...\")\n",
    "\n",
    "o3_prompt = (\n",
    "    \"ä»¥ä¸‹ã¯è«–æ–‡ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ–‡å­—èµ·ã“ã—ã—ãŸå†…å®¹ã§ã™ã€‚\\n\"\n",
    "    \"å†…å®¹ã«ã¤ã„ã¦ã€o3ã®è¦–ç‚¹ã‹ã‚‰æ„è¦‹ã‚„æ”¹å–„ç‚¹ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚\\n\\n\"\n",
    ")\n",
    "o3_document = o3_prompt + transcription_text\n",
    "\n",
    "# o3ç”¨æ–‡æ›¸ã‚’ä¿å­˜\n",
    "o3_prompt_path = os.path.join(output_dir, \"o3_prompt_document.txt\")\n",
    "with open(o3_prompt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(o3_document)\n",
    "\n",
    "print(\"âœ… o3ç”¨ã®æ–‡æ›¸ä½œæˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")\n",
    "\n",
    "# -------------------------\n",
    "# å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸\n",
    "# -------------------------\n",
    "print(\"ğŸ‰ ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42f573e-84b4-46b6-b34a-77fc16aecdf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
